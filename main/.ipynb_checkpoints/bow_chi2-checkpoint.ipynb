{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0b4f3b9",
   "metadata": {},
   "source": [
    "6 comb\n",
    "\n",
    "* vectorization: \n",
    "    a. bow\n",
    "    b. tfidf\n",
    "    * feature selection\n",
    "        a. chi2\n",
    "        b. f-test\n",
    "        c. mi\n",
    "        models:\n",
    "            # logistic: 1008 comb\n",
    "            # svm: 2688\n",
    "            # rf: 8000\n",
    "            \n",
    "            \n",
    "total: 82560 combs\n",
    "lots of 5 cross validation = g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd7dc56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import re\n",
    "from data_cleaning import *\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e185d1",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1da73d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "train_data = pd.read_csv(\"Train.csv\", sep=',')\n",
    "test_data = pd.read_csv(\"Test.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a2808e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating instance and label for Train\n",
    "X_train_raw = [x[0] for x in train_data[['text']].values]\n",
    "Y_train = [x[0] for x in train_data[['sentiment']].values]\n",
    "X_test_raw = [x[0] for x in test_data[['text']].values]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e0b5c2",
   "metadata": {},
   "source": [
    "#### (1). data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e8b8cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. data cleaning (optional)\n",
    "X_train_need_to_clean = pd.DataFrame(X_train_raw)\n",
    "X_test_need_to_clean = pd.DataFrame(X_test_raw)\n",
    "\n",
    "# remove url, # and @\n",
    "X_train_need_to_clean.replace(\"\\b*https?:\\S*\", '', regex=True, inplace=True)\n",
    "X_train_need_to_clean.replace(\"\\b*@\\S*\", '', regex=True, inplace=True)\n",
    "X_train_need_to_clean.replace(\"\\b*#\\S*\", '', regex=True, inplace=True)\n",
    "X_test_need_to_clean.replace(\"\\b*https?:\\S*\", '', regex=True, inplace=True)\n",
    "X_test_need_to_clean.replace(\"\\b*@\\S*\", '', regex=True, inplace=True)\n",
    "X_test_need_to_clean.replace(\"\\b*#\\S*\", '', regex=True, inplace=True)\n",
    "\n",
    "for i in range(X_train_need_to_clean.shape[0]):\n",
    "    X_train_need_to_clean.loc[i, 0] = ' '.join(text_preprocessing(X_train_need_to_clean.loc[i, 0], remove_html=False))\n",
    "\n",
    "for i in range(X_test_need_to_clean.shape[0]):\n",
    "    X_test_need_to_clean.loc[i, 0] = ' '.join(text_preprocessing(X_test_need_to_clean.loc[i, 0], remove_html=False))    \n",
    "\n",
    "X_train_need_to_clean.replace('[^\\w\\s]','', regex=True, inplace=True)\n",
    "X_test_need_to_clean.replace('[^\\w\\s]','', regex=True, inplace=True)\n",
    "    \n",
    "    \n",
    "X_train_clean = [x[0] for x in X_train_need_to_clean[[0]].values]\n",
    "X_test_clean = [x[0] for x in X_test_need_to_clean[[0]].values]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d505cfc6",
   "metadata": {},
   "source": [
    "#### (2). vectorization (transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80e9b061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag of words\n",
    "# countvectorizer\n",
    "BoW_vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "\n",
    "X_train_BoW = BoW_vectorizer.fit_transform(X_train_clean)\n",
    "X_test_BoW = BoW_vectorizer.transform(X_test_clean)\n",
    "\n",
    "X_train_tranformed, X_test_transformed = X_train_BoW, X_test_BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "728224a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFIDF\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(2,2))\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_clean)\n",
    "\n",
    "\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_raw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00eb209",
   "metadata": {},
   "source": [
    "### comparison between data with preprocessing and without preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efa9e23a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21802, 204657)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CountVectorizer(ngram_range=(2,2)).fit_transform(X_train_raw).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0ba93d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21802, 127482)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tranformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df12cbb",
   "metadata": {},
   "source": [
    "# 2. feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8caae81",
   "metadata": {},
   "source": [
    "### type of selector (choose one of them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8b4469c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chi2     \n",
    "selector = SelectKBest(chi2,k=5000)\n",
    "X_train_new = selector.fit_transform(X_train_tranformed,Y_train)\n",
    "X_test_new = selector.transform(X_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9216c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_classifs\n",
    "selector = SelectKBest(f_classif,k=5000)\n",
    "X_train_new = selector.fit_transform(X_train_tranformed,Y_train)\n",
    "X_test_new = selector.transform(X_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eb66a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mutual_info_classif\n",
    "selector = SelectKBest(mutual_info_classif,k=10000)\n",
    "X_train_new = selector.fit_transform(X_train_tranformed,Y_train)\n",
    "X_test_new = selector.transform(X_test_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5670eb1c",
   "metadata": {},
   "source": [
    "# 3. training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a51c48",
   "metadata": {},
   "source": [
    "### split data for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d3dbc072",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = X_train_tranformed.shape[0]\n",
    "test_size = X_test_transformed.shape[0]\n",
    "## random hold out\n",
    "ts = test_size/train_size\n",
    "X_train_s, X_validation, y_train_s, y_validation = train_test_split(X_train_new,Y_train, test_size=ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "72b58f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = round(X_train_s.shape[0] / X_validation.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cd4222",
   "metadata": {},
   "source": [
    "#### Prototype selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fc1960fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "X_resampled, y_resampled = rus.fit_resample(X_train_s, y_train_s)\n",
    "X_train_s, y_train_s = X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4fa56ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    2668\n",
       "neutral     2668\n",
       "positive    2668\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resampled_labels = pd.DataFrame(y_resampled)\n",
    "resampled_labels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2b7691",
   "metadata": {},
   "source": [
    "#### Prototype generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3a919b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import ClusterCentroids\n",
    "cc = ClusterCentroids(random_state=0, n_jobs=-1)\n",
    "X_resampled, y_resampled = cc.fit_resample(X_train_s, y_train_s)\n",
    "X_train_s, y_train_s = X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d76f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_labels = pd.DataFrame(y_resampled)\n",
    "resampled_labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9146ea07",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = round(X_train_s.shape[0] / X_validation.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbce0ac1",
   "metadata": {},
   "source": [
    "## base model: 0R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "77aa3160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base model score:  0.5806348041464086\n"
     ]
    }
   ],
   "source": [
    "clf = DummyClassifier(strategy='most_frequent')\n",
    "basemodel = clf.fit(X_train_raw, Y_train)\n",
    "print(\"base model score: \", basemodel.score(X_train_raw, Y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a3d04b",
   "metadata": {},
   "source": [
    "## linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "daf054f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'positive'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19792/2128746101.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;34m'normalize'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m }\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mlinear_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_validation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\laizh\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    667\u001b[0m             \u001b[0msample_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 669\u001b[1;33m         X, y, X_offset, y_offset, X_scale = self._preprocess_data(\n\u001b[0m\u001b[0;32m    670\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\laizh\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36m_preprocess_data\u001b[1;34m(X, y, fit_intercept, normalize, copy, sample_weight, return_mean, check_input)\u001b[0m\n\u001b[0;32m    252\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"K\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 254\u001b[1;33m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfit_intercept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\laizh\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order, like)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_asarray_with_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'positive'"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "linear_hyper = {\n",
    "    'normalize': [True, False]\n",
    "}\n",
    "linear_model = LinearRegression().fit(X_train_s, y_train_s)\n",
    "linear_model.score(X_validation, y_validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0cc0edb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae133eeb",
   "metadata": {},
   "source": [
    "## naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5901ac35",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19792/3186081346.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# gussian\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mgnb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mgnb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mgnb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_validation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Gaussian: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgnb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_validation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\laizh\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    243\u001b[0m         \"\"\"\n\u001b[0;32m    244\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 245\u001b[1;33m         return self._partial_fit(\n\u001b[0m\u001b[0;32m    246\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_refit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m         )\n",
      "\u001b[1;32mD:\\Users\\laizh\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36m_partial_fit\u001b[1;34m(self, X, y, classes, _refit, sample_weight)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m         \u001b[0mfirst_call\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_partial_fit_first_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfirst_call\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    403\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m             \u001b[0msample_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\laizh\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    579\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\laizh\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    962\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y cannot be None\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    963\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 964\u001b[1;33m     X = check_array(\n\u001b[0m\u001b[0;32m    965\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\laizh\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    718\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    719\u001b[0m         \u001b[0m_ensure_no_complex_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 720\u001b[1;33m         array = _ensure_sparse_format(\n\u001b[0m\u001b[0;32m    721\u001b[0m             \u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    722\u001b[0m             \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\laizh\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_ensure_sparse_format\u001b[1;34m(spmatrix, accept_sparse, dtype, copy, force_all_finite, accept_large_sparse)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maccept_sparse\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m         raise TypeError(\n\u001b[0m\u001b[0;32m    441\u001b[0m             \u001b[1;34m\"A sparse matrix was passed, but dense \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m             \u001b[1;34m\"data is required. Use X.toarray() to \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array."
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "\n",
    "# gussian\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train_s, y_train_s)\n",
    "gnb.score(X_validation, y_validation)\n",
    "print(\"Gaussian: \", gnb.score(X_validation, y_validation))\n",
    "\n",
    "# multinomial\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_s, y_train_s)\n",
    "mnb.score(X_validation, y_validation)\n",
    "print(\"multinomial: \", mnb.score(X_validation, y_validation))\n",
    "\n",
    "# complement\n",
    "cnb = ComplementNB()\n",
    "cnb.fit(X_train_s, y_train_s)\n",
    "cnb.score(X_validation, y_validation)\n",
    "print(\"complement: \", cnb.score(X_validation, y_validation))\n",
    "\n",
    "# bernoulli\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(X_train_s, y_train_s)\n",
    "print(\"bernoulli: \", bnb.score(X_validation, y_validation))\n",
    "\n",
    "# categorical\n",
    "canb = CategoricalNB()\n",
    "canb.fit(X_train_s, y_train_s)\n",
    "canb.score(X_validation, y_validation)\n",
    "print(\"categorical: \", canb.score(X_validation, y_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44742a37",
   "metadata": {},
   "source": [
    "## decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "32eaeedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_hyper = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': range(2,20,2),\n",
    "    'min_samples_leaf': range(1,10,1),\n",
    "    'max_features': ['auto', 'log2']\n",
    "}\n",
    "\n",
    "# random method\n",
    "search_dt = RandomizedSearchCV(DecisionTreeClassifier(), dt_hyper, scoring='accuracy', cv=cv, n_iter=800, n_jobs=-1)\n",
    "dt_result = search_dt.fit(X_train_s, y_train_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eff26af",
   "metadata": {},
   "source": [
    "## logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "90ef554f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'solver': 'lbfgs', 'penalty': 'none', 'multi_class': 'ovr', 'max_iter': 200, 'C': 0.001}\n",
      "0.6213166897766308\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logi_hyper = {\n",
    "    'solver': ['newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "    'penalty': ['l1', 'l2', 'none', 'elasticnet'],\n",
    "    'C': [1e-3, 1e-2, 1e-1, 1, 10, 100, 1000],\n",
    "    'max_iter': [100, 200, 300],\n",
    "    'multi_class': ['auto', 'ovr', 'multinomial']\n",
    "}\n",
    "\n",
    "search_logi = RandomizedSearchCV(LogisticRegression(n_jobs=-1),logi_hyper, scoring='f1_weighted', cv=cv, n_iter=101, n_jobs=-1)\n",
    "logi_result = search_logi.fit(X_train_s, y_train_s)\n",
    "\n",
    "pd.DataFrame(logi_result.cv_results_)\n",
    "print(logi_result.best_params_)\n",
    "print(logi_result.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28babce",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3938f4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_iter': 1000, 'kernel': 'rbf', 'gamma': 1, 'degree': 10, 'decision_function_shape': 'ovo', 'C': 0.001}\n",
      "0.513977971578384\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_hyper = {\n",
    "    'degree': [3, 5, 10, 15],\n",
    "    'gamma': [1,0.1,0.01,0.001],\n",
    "    'C': [1e-3, 1e-2, 1e-1, 1, 10, 100, 1000],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmod'],\n",
    "    'max_iter': [100, 500, 1000],\n",
    "    'decision_function_shape': ['ovo', 'ovr']\n",
    "}\n",
    "\n",
    "# random method\n",
    "search_svm = RandomizedSearchCV(SVC(), svm_hyper, scoring='f1_weighted', cv=cv, n_iter=269)\n",
    "svm_result = search_svm.fit(X_train_s, y_train_s)\n",
    "\n",
    "pd.DataFrame(svm_result.cv_results_)\n",
    "print(svm_result.best_params_)\n",
    "print(svm_result.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519020a7",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffd99d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_hyper = {\n",
    "    'n_estimators': [90, 100, 115 , 130],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': range(2,20,2),\n",
    "    'min_samples_leaf': range(1,10,1),\n",
    "    'min_samples_split': range(2,10,2),\n",
    "    'max_features': ['auto', 'log2']\n",
    "}\n",
    "\n",
    "# random method\n",
    "search_rf = RandomizedSearchCV(RandomForestClassifier(n_jobs=-1), rf_hyper, scoring='f1_weighted', cv=cv, n_iter=800, n_jobs=-1)\n",
    "rf_result = search_rf.fit(X_train_s, y_train_s)\n",
    "\n",
    "pd.DataFrame(rf_result.cv_results_)\n",
    "print(rf_result.best_params_)\n",
    "print(rf_result.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f316425",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f3740c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier(criterion=rf_result.best_params_['criterion'], max_depth=rf_result.best_params_['max_depth'],\n",
    "                                  max_features='log2', min_samples_leaf=rf_result.best_params_['min_samples_leaf'], \n",
    "                                  min_samples_split=rf_result.best_params_['min_samples_split'], n_estimators=rf_result.best_params_['n_estimators'],\n",
    "                                  random_state=0).fit(X_train_s, y_train_s)\n",
    "\n",
    "svm_model = SVC(degree=svm_result.best_params_['degree'], gamma=svm_result.best_params_['gamma'] ,C=svm_result.best_params_['C'], \n",
    "                kernel=svm_result.best_params_['kernel'], max_iter=svm_result.best_params_['max_iter'], \n",
    "                decision_function_shape=svm_result.best_params_['decision_function_shape']).fit(X_train_s, y_train_s)\n",
    "\n",
    "logi_model = LogisticRegression(solver=logi_result.best_params_['solver'], penalty=logi_result.best_params_['penalty'],\n",
    "                                C=logi_result.best_params_['C'], max_iter=logi_result.best_params_['max_iter'],\n",
    "                                multi_class=logi_result.best_params_['multi_class']).fit(X_train_s, y_train_s)\n",
    "\n",
    "estimators = [('rf', rf_model),('svr', svm_model), ('log', logi_model)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580c2e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_stacking = StackingClassifier(estimators=estimators, final_estimator=DecisionTreeClassifier(), n_jobs=-1).fit(X_train_s, y_train_s)\n",
    "dt_stacking.score(X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d24ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_stacking = StackingClassifier(estimators=estimators, final_estimator=SVC(), n_jobs=-1).fit(X_train_s, y_train_s)\n",
    "svm_stacking.score(X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9440ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "logi_stacking = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(max_iter=200), n_jobs=-1).fit(X_train_s, y_train_s)\n",
    "logi_stacking.score(X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e355a709",
   "metadata": {},
   "source": [
    "# 4. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17893cbe",
   "metadata": {},
   "source": [
    "### score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8fd5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4cb991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will take the model and datas, print the evaluation scores and output the precdtion\n",
    "\n",
    "def evaluate_model(model_name: str, model, X_validation, y_validation, X_test_new, cv=5):\n",
    "    # basestic\n",
    "    print(f\"{model_name}: \")\n",
    "\n",
    "    # validation acc\n",
    "    vc = np.mean(cross_val_score(model,X_validation, y_validation, cv=cv))\n",
    "    print(\"Validation accuracy: \", vc)\n",
    "\n",
    "    # predictions for test data\n",
    "    prediction = model.predict(X_test_new)\n",
    "\n",
    "    validation_label = model.predict(X_validation)\n",
    "\n",
    "    # macro metrics\n",
    "    precision_m = precision_score(y_validation,validation_label, average = 'macro')\n",
    "    print('macro precision is ', precision_m)\n",
    "\n",
    "    recall_m = recall_score(y_validation,validation_label, average = 'macro')\n",
    "    print('macro recall is ', recall_m)\n",
    "\n",
    "    # f1\n",
    "    f1_m = f1_score(y_validation,validation_label, average = 'macro')\n",
    "    print('macro f1 is ', f1_m)\n",
    "\n",
    "\n",
    "    # weighted metrics\n",
    "    precision_w = precision_score(y_validation,validation_label, average = 'weighted')\n",
    "    print('weighted precision is ', precision_w)\n",
    "\n",
    "    recall_w = recall_score(y_validation,validation_label, average = 'weighted')\n",
    "    print('weighted recall is ', recall_w)\n",
    "\n",
    "    # f1\n",
    "    f1_w = f1_score(y_validation,validation_label, average = 'weighted')\n",
    "    print('weighted f1 is ', f1_w)\n",
    "    print()\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10bdb3b",
   "metadata": {},
   "source": [
    "## basic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a1ea89",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(\"basemodel 0R\", basemodel, X_validation, y_validation,X_test_new)\n",
    "# evaluate_model(\"linear regression\", linear_model, X_validation, y_validation,X_test_new)\n",
    "# evaluate_model(\"linear regression\", linear_model, X_validation, y_validation,X_test_new)hua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1931e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logi_label = evaluate_model(\"Logistic Regression\", logi_model, X_validation, y_validation,X_test_new)\n",
    "s_label = evaluate_model(\"SVM\", svm_model, X_validation, y_validation,X_test_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760b094e",
   "metadata": {},
   "source": [
    "### bagging models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65d8f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_label = evaluate_model(\"random forest\", rf_model, X_validation, y_validation,X_test_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a62a7c",
   "metadata": {},
   "source": [
    "### stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bd8fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_label = evaluate_model(\"svm_stacking\", svm_stacking, X_validation, y_validation,X_test_new)\n",
    "dts_label = evaluate_model(\"dt_stacking\", dt_stacking, X_validation, y_validation,X_test_new)\n",
    "logi_s_label = evaluate_model(\"logi_stacking\", logi_stacking, X_validation, y_validation,X_test_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d208a42",
   "metadata": {},
   "source": [
    "# 5. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87e1253",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f23361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "plot_confusion_matrix(logi_model, X_validation, y_validation)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd921f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm\n",
    "plot_confusion_matrix(svm_model, X_validation, y_validation)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab8fcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest\n",
    "plot_confusion_matrix(rf_model, X_validation, y_validation)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58de3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm_stacking\n",
    "plot_confusion_matrix(svm_stacking, X_validation, y_validation)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5a91eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt_stacking\n",
    "plot_confusion_matrix(dt_stacking, X_validation, y_validation)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b993faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logi_stacking\n",
    "plot_confusion_matrix(logi_stacking, X_validation, y_validation)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e31db14",
   "metadata": {},
   "source": [
    "## output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "efc4cd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "prediction_label = logi_s_label\n",
    "with open('prediction.csv','w') as output:\n",
    "    output.write(\"id,sentiment\\n\")\n",
    "    for i in range(0,len(prediction_label)):\n",
    "        output.write(str(test_data['id'].iloc[i]))\n",
    "        output.write(\",\")\n",
    "        output.write(prediction_label[i])\n",
    "        output.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a3763faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7971"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a708f493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8079x5000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 11927 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e09044",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
