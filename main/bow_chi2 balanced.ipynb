{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0b4f3b9",
   "metadata": {},
   "source": [
    "6 comb\n",
    "\n",
    "* vectorization: \n",
    "    a. bow\n",
    "    b. tfidf\n",
    "    * feature selection\n",
    "        a. chi2\n",
    "        b. f-test\n",
    "        c. mi\n",
    "        models:\n",
    "            # logistic: 1008 comb\n",
    "            # svm: 2688\n",
    "            # rf: 8000\n",
    "            \n",
    "            \n",
    "total: 82560 combs\n",
    "lots of 5 cross validation = g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fd7dc56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import re\n",
    "from data_cleaning import *\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e185d1",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d1da73d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "train_data = pd.read_csv(\"Train.csv\", sep=',')\n",
    "test_data = pd.read_csv(\"Test.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4a2808e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating instance and label for Train\n",
    "X_train_raw = [x[0] for x in train_data[['text']].values]\n",
    "Y_train = [x[0] for x in train_data[['sentiment']].values]\n",
    "X_test_raw = [x[0] for x in test_data[['text']].values]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e0b5c2",
   "metadata": {},
   "source": [
    "#### (1). data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6e8b8cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. data cleaning (optional)\n",
    "X_train_need_to_clean = pd.DataFrame(X_train_raw)\n",
    "X_test_need_to_clean = pd.DataFrame(X_test_raw)\n",
    "\n",
    "# remove url, # and @\n",
    "X_train_need_to_clean.replace(\"\\b*https?:\\S*\", ' ', regex=True, inplace=True)\n",
    "X_train_need_to_clean.replace(\"\\b*@\\S*\", ' ', regex=True, inplace=True)\n",
    "# X_train_need_to_clean.replace(\"\\b*#\\S*\", ' ', regex=True, inplace=True)\n",
    "X_test_need_to_clean.replace(\"\\b*https?:\\S*\", ' ', regex=True, inplace=True)\n",
    "X_test_need_to_clean.replace(\"\\b*@\\S*\", ' ', regex=True, inplace=True)\n",
    "# X_test_need_to_clean.replace(\"\\b*#\\S*\", ' ', regex=True, inplace=True)\n",
    "\n",
    "\n",
    "X_train_need_to_clean.replace('[^\\w\\s]','', regex=True, inplace=True)\n",
    "X_test_need_to_clean.replace('[^\\w\\s]','', regex=True, inplace=True)\n",
    "    \n",
    "    \n",
    "X_train_clean = [x[0] for x in X_train_need_to_clean[[0]].values]\n",
    "X_test_clean = [x[0] for x in X_test_need_to_clean[[0]].values]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d505cfc6",
   "metadata": {},
   "source": [
    "#### (2). vectorization (transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "728224a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFIDF\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(2,2))\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_clean)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_clean)\n",
    "X_train_tranformed, X_test_transformed = X_train_tfidf, X_test_tfidf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df12cbb",
   "metadata": {},
   "source": [
    "# 2. feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8caae81",
   "metadata": {},
   "source": [
    "### type of selector (choose one of them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8b4469c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chi2     \n",
    "selector = SelectKBest(chi2,k=10000)\n",
    "X_train_new = selector.fit_transform(X_train_tranformed,Y_train)\n",
    "X_test_new = selector.transform(X_test_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5670eb1c",
   "metadata": {},
   "source": [
    "# 3. training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a51c48",
   "metadata": {},
   "source": [
    "### split data for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d3dbc072",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = X_train_tranformed.shape[0]\n",
    "test_size = X_test_transformed.shape[0]\n",
    "## random hold out\n",
    "ts = test_size/train_size\n",
    "X_train_, X_validation, y_train_, y_validation  = train_test_split(X_train_new,Y_train, test_size=ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cd4222",
   "metadata": {},
   "source": [
    "### Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "fc1960fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler, NearMiss\n",
    "sampling_strategy = {'negative':2662, 'positive':2500, 'neutral': 2000}\n",
    "rus = RandomUnderSampler(sampling_strategy=sampling_strategy,random_state=0)\n",
    "X_resampled, y_resampled = rus.fit_resample(X_train_s, y_train_s)\n",
    "X_train_s, y_train_s = X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "4fa56ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    2662\n",
       "positive    2500\n",
       "neutral     2000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resampled_labels = pd.DataFrame(y_train_s)\n",
    "resampled_labels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edce3772",
   "metadata": {},
   "source": [
    "### oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5e2b1225",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_, y_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2ddc5069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    9144\n",
       "neutral     9144\n",
       "positive    9144\n",
       "dtype: int64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resampled_labels = pd.DataFrame(y_train_smote)\n",
    "resampled_labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "526fc4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_s, y_train_s = X_train_, y_train_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db053cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = round(X_train_s.shape[0] / X_validation.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbce0ac1",
   "metadata": {},
   "source": [
    "## base model: 0R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "77aa3160",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base model score:  0.5806348041464086\n"
     ]
    }
   ],
   "source": [
    "clf = DummyClassifier(strategy='most_frequent')\n",
    "basemodel = clf.fit(X_train_raw, Y_train)\n",
    "print(\"base model score: \", basemodel.score(X_train_raw, Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a97e255e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d974db3",
   "metadata": {},
   "source": [
    "## naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "413066e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian:  0.7696343662895556\n",
      "multinomial:  0.5958353828496474\n",
      "complement:  0.484833579275291\n",
      "bernoulli:  0.6835546810952615\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "\n",
    "# gussian\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train_s.toarray(), y_train_s)\n",
    "gnb.score(X_validation.toarray(), y_validation)\n",
    "print(\"Gaussian: \", gnb.score(X_validation.toarray(), y_validation))\n",
    "\n",
    "# multinomial\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_s.toarray(), y_train_s)\n",
    "mnb.score(X_validation.toarray(), y_validation)\n",
    "print(\"multinomial: \", mnb.score(X_validation.toarray(), y_validation))\n",
    "\n",
    "# complement\n",
    "cnb = ComplementNB()\n",
    "cnb.fit(X_train_s.toarray(), y_train_s)\n",
    "cnb.score(X_validation.toarray(), y_validation)\n",
    "print(\"complement: \", cnb.score(X_validation.toarray(), y_validation))\n",
    "\n",
    "# bernoulli\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(X_train_s.toarray(), y_train_s)\n",
    "print(\"bernoulli: \", bnb.score(X_validation.toarray(), y_validation))\n",
    "\n",
    "# # categorical\n",
    "# canb = CategoricalNB()\n",
    "# canb.fit(X_train_s.toarray(), y_train_s)\n",
    "# canb.score(X_validation.toarray(), y_validation)\n",
    "# print(\"categorical: \", canb.score(X_validation.toarray(), y_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ce5e9f",
   "metadata": {},
   "source": [
    "## decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fed7ac53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45616441931138035"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_hyper = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': range(2,20,2),\n",
    "    'min_samples_leaf': range(1,10,1),\n",
    "    'max_features': ['auto', 'log2']\n",
    "}\n",
    "\n",
    "# random method\n",
    "search_dt = RandomizedSearchCV(DecisionTreeClassifier(), dt_hyper, scoring='f1_weighted', cv=cv, n_iter=800, n_jobs=-1)\n",
    "dt_result = search_dt.fit(X_train_s, y_train_s)\n",
    "dt_result.best_params_\n",
    "dt_result.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d69c0d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45616441931138035"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_result.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "035b998b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min_samples_leaf': 2,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': 16,\n",
       " 'criterion': 'gini'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d08261ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model = DecisionTreeClassifier(criterion= dt_result.best_params_['criterion'], max_depth=dt_result.best_params_['max_depth'],\n",
    "                                  min_samples_leaf=dt_result.best_params_['min_samples_leaf'],\n",
    "                                  max_features=dt_result.best_params_['max_features']).fit(X_train_s, y_train_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eff26af",
   "metadata": {},
   "source": [
    "## logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "90ef554f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'solver': 'lbfgs', 'penalty': 'none', 'multi_class': 'ovr', 'max_iter': 100, 'C': 0.01}\n",
      "0.6212483408429171\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logi_hyper = {\n",
    "    'solver': ['newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "    'penalty': ['l1', 'l2', 'none', 'elasticnet'],\n",
    "    'C': [1e-3, 1e-2, 1e-1, 1, 10, 100, 1000],\n",
    "    'max_iter': [100, 300, 500],\n",
    "    'multi_class': ['auto', 'ovr', 'multinomial']\n",
    "}\n",
    "\n",
    "search_logi = RandomizedSearchCV(LogisticRegression(n_jobs=-1),logi_hyper, scoring='f1_weighted', cv=cv, n_iter=150, n_jobs=-1)\n",
    "logi_result = search_logi.fit(X_train_s, y_train_s)\n",
    "\n",
    "print(logi_result.best_params_)\n",
    "print(logi_result.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9feb6b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     9112\n",
       "positive    3947\n",
       "negative    2644\n",
       "dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(y_train_s).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28babce",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3938f4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_iter': 1000, 'kernel': 'rbf', 'gamma': 1, 'degree': 3, 'decision_function_shape': 'ovr', 'C': 1000}\n",
      "0.3879547141125343\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_hyper = {\n",
    "    'degree': [3, 5, 10, 15],\n",
    "    'gamma': [1,0.1,0.01,0.001],\n",
    "    'C': [1e-3, 1e-2, 1e-1, 1, 10, 100, 1000],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmod'],\n",
    "    'max_iter': [100, 500, 1000],\n",
    "    'decision_function_shape': ['ovo', 'ovr']\n",
    "}\n",
    "\n",
    "# random method\n",
    "search_svm = RandomizedSearchCV(SVC(), svm_hyper, scoring='f1_weighted', cv=cv, n_iter=200)\n",
    "svm_result = search_svm.fit(X_train_s, y_train_s)\n",
    "\n",
    "pd.DataFrame(svm_result.cv_results_)\n",
    "print(svm_result.best_params_)\n",
    "print(svm_result.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519020a7",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dffd99d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_928/158908626.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# random method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0msearch_rf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrf_hyper\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'f1_weighted'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m800\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mrf_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msearch_rf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrf_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\laizh\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    889\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 891\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\laizh\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1764\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1765\u001b[0m         \u001b[1;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1766\u001b[1;33m         evaluate_candidates(\n\u001b[0m\u001b[0;32m   1767\u001b[0m             ParameterSampler(\n\u001b[0;32m   1768\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\laizh\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    836\u001b[0m                     )\n\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 838\u001b[1;33m                 out = parallel(\n\u001b[0m\u001b[0;32m    839\u001b[0m                     delayed(_fit_and_score)(\n\u001b[0;32m    840\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\laizh\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1055\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1056\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1057\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1058\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\laizh\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    933\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 935\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    936\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    937\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\laizh\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\laizh\\anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    438\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\laizh\\anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_hyper = {\n",
    "    'n_estimators': [90, 100, 115 , 130],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': range(2,20,2),\n",
    "    'min_samples_leaf': range(1,10,1),\n",
    "    'min_samples_split': range(2,10,2),\n",
    "    'max_features': ['auto', 'log2']\n",
    "}\n",
    "\n",
    "# random method\n",
    "search_rf = RandomizedSearchCV(RandomForestClassifier(n_jobs=-1), rf_hyper, scoring='f1_weighted', cv=cv, n_iter=800, n_jobs=-1)\n",
    "rf_result = search_rf.fit(X_train_s, y_train_s)\n",
    "\n",
    "pd.DataFrame(rf_result.cv_results_)\n",
    "print(rf_result.best_params_)\n",
    "print(rf_result.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f316425",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f3740c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "rf_model = RandomForestClassifier(criterion=rf_result.best_params_['criterion'], max_depth=rf_result.best_params_['max_depth'],\n",
    "                                  max_features='log2', min_samples_leaf=rf_result.best_params_['min_samples_leaf'], \n",
    "                                  min_samples_split=rf_result.best_params_['min_samples_split'], n_estimators=rf_result.best_params_['n_estimators'],\n",
    "                                  random_state=0, class_weight='balanced_subsample').fit(X_train_s, y_train_s)\n",
    "\n",
    "# svm_model = SVC(degree=svm_result.best_params_['degree'], gamma=svm_result.best_params_['gamma'] ,C=svm_result.best_params_['C'], \n",
    "#                 kernel=svm_result.best_params_['kernel'], max_iter=svm_result.best_params_['max_iter'], \n",
    "#                 decision_function_shape=svm_result.best_params_['decision_function_shape']).fit(X_train_s, y_train_s)\n",
    "\n",
    "svm_model = SVC( class_weight='balanced').fit(X_train_s, y_train_s)\n",
    "\n",
    "logi_model = LogisticRegression(solver=logi_result.best_params_['solver'], penalty=logi_result.best_params_['penalty'],\n",
    "                                C=logi_result.best_params_['C'], max_iter=logi_result.best_params_['max_iter'],\n",
    "                                multi_class=logi_result.best_params_['multi_class'], ).fit(X_train_s, y_train_s)\n",
    "\n",
    "estimators = [('rf', rf_model),('svr', svm_model), ('log', logi_model),('dt', dt_model)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580c2e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_stacking = StackingClassifier(estimators=estimators, final_estimator=DecisionTreeClassifier(), n_jobs=-1).fit(X_train_s, y_train_s)\n",
    "dt_stacking.score(X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d24ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_stacking = StackingClassifier(estimators=estimators, final_estimator=SVC(), n_jobs=-1).fit(X_train_s, y_train_s)\n",
    "svm_stacking.score(X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9440ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "logi_stacking = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(max_iter=200), n_jobs=-1).fit(X_train_s, y_train_s)\n",
    "logi_stacking.score(X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82205de0",
   "metadata": {},
   "source": [
    "super_estimator = [('ls', logi_stacking), ('ss', svm_stacking), ('ds', dt_stacking),('log', logi_model) ]\n",
    "super_stacking = StackingClassifier(estimators=super_estimator, final_estimator=LogisticRegression(max_iter=200,n_jobs=-1)).fit(X_train_s, y_train_s)\n",
    "super_stacking.score(X_validation, y_validation)                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e355a709",
   "metadata": {},
   "source": [
    "# 4. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17893cbe",
   "metadata": {},
   "source": [
    "### score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4cb991",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "from mlxtend.evaluate import bias_variance_decomp\n",
    "\n",
    "# this function will take the model and datas, print the evaluation scores and output the precdtion\n",
    "\n",
    "def evaluate_model(model_name: str, model, X_validation, y_validation, X_test_new, cv=5):\n",
    "    # basestic\n",
    "    print(f\"{model_name}: \")\n",
    "\n",
    "    # validation acc\n",
    "    validation_scores = cross_val_score(model,X_validation, y_validation, cv=cv)\n",
    "    vc = np.mean(validation_scores)\n",
    "    print(\"Validation accuracy: \", validation_scores)\n",
    "    \n",
    "    print(\"validation acc for each cross:\", )\n",
    "\n",
    "    # predictions for test data\n",
    "    prediction = model.predict(X_test_new)\n",
    "\n",
    "    validation_label = model.predict(X_validation)\n",
    "    \n",
    "    # individual metrics\n",
    "    precision_m = precision_score(y_validation,validation_label, average = None)\n",
    "    print('individual precision is ', precision_m)\n",
    "\n",
    "    recall_m = recall_score(y_validation,validation_label, average = None)\n",
    "    print('individual recall is ', recall_m)\n",
    "\n",
    "    # f1\n",
    "    f1_m = f1_score(y_validation,validation_label, average = None)\n",
    "    print('individual f1 is ', f1_m)\n",
    "    \n",
    "    # macro metrics\n",
    "    precision_m = precision_score(y_validation,validation_label, average = 'macro')\n",
    "    print('macro precision is ', precision_m)\n",
    "\n",
    "    recall_m = recall_score(y_validation,validation_label, average = 'macro')\n",
    "    print('macro recall is ', recall_m)\n",
    "\n",
    "    # f1\n",
    "    f1_m = f1_score(y_validation,validation_label, average = 'macro')\n",
    "    print('macro f1 is ', f1_m)\n",
    "\n",
    "\n",
    "    # weighted metrics\n",
    "    precision_w = precision_score(y_validation,validation_label, average = 'weighted')\n",
    "    print('weighted precision is ', precision_w)\n",
    "\n",
    "    recall_w = recall_score(y_validation,validation_label, average = 'weighted')\n",
    "    print('weighted recall is ', recall_w)\n",
    "\n",
    "    # f1\n",
    "    f1_w = f1_score(y_validation,validation_label, average = 'weighted')\n",
    "    print('weighted f1 is ', f1_w)\n",
    "\n",
    "    \n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10bdb3b",
   "metadata": {},
   "source": [
    "## basic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517eede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_label = evaluate_model(\"basemodel 0R\", basemodel, X_validation, y_validation,X_test_new)\n",
    "dt_label = evaluate_model(\"decision tree\", dt_model,X_validation, y_validation,X_test_new)\n",
    "# print(evaluate_model(\"guassian nb\", gnb, X_validation.toarray(), y_validation_numberise, X_test_new.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1931e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "logi_label = evaluate_model(\"Logistic Regression\", logi_model, X_validation, y_validation,X_test_new)\n",
    "s_label = evaluate_model(\"SVM\", svm_model, X_validation, y_validation,X_test_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760b094e",
   "metadata": {},
   "source": [
    "### bagging models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65d8f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_label = evaluate_model(\"random forest\", rf_model, X_validation, y_validation,X_test_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a62a7c",
   "metadata": {},
   "source": [
    "### stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bd8fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_label = evaluate_model(\"svm_stacking\", svm_stacking, X_validation, y_validation,X_test_new)\n",
    "dts_label = evaluate_model(\"dt_stacking\", dt_stacking, X_validation, y_validation,X_test_new)\n",
    "logi_s_label = evaluate_model(\"logi_stacking\", logi_stacking, X_validation, y_validation,X_test_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9ff8be",
   "metadata": {},
   "source": [
    "evaluate_model(\"super stacking\", super_stacking, X_validation, y_validation,X_test_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa76bea1",
   "metadata": {},
   "source": [
    "plot_confusion_matrix(super_stacking, X_validation, y_validation)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d208a42",
   "metadata": {},
   "source": [
    "# 5. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87e1253",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5d8cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = gnb.predict(X_validation.toarray())\n",
    "print(y_pred)\n",
    "# cm = confusion_matrix()\n",
    "# plot_confusion_matrix(y_validation, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e79084",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_pred).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6451ad6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f23361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "plot_confusion_matrix(logi_model, X_validation, y_validation)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd921f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm\n",
    "plot_confusion_matrix(svm_model, X_validation, y_validation)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab8fcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest\n",
    "plot_confusion_matrix(rf_model, X_validation, y_validation)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58de3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm_stacking\n",
    "plot_confusion_matrix(svm_stacking, X_validation, y_validation)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5a91eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt_stacking\n",
    "plot_confusion_matrix(dt_stacking, X_validation, y_validation)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b993faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logi_stacking\n",
    "plot_confusion_matrix(logi_stacking, X_validation, y_validation)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5943e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(logi_label).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e31db14",
   "metadata": {},
   "source": [
    "## output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b18ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "prediction_label = dts_label\n",
    "with open('prediction.csv','w') as output:\n",
    "    output.write(\"id,sentiment\\n\")\n",
    "    for i in range(0,len(prediction_label)):\n",
    "        output.write(str(test_data['id'].iloc[i]))\n",
    "        output.write(\",\")\n",
    "        output.write(prediction_label[i])\n",
    "        output.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636aa1cf",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# k fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ad4969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_validate\n",
    "# _scoring = ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted']\n",
    "\n",
    "\n",
    "# logi_model = LogisticRegression(solver=logi_result.best_params_['solver'], penalty=logi_result.best_params_['penalty'],\n",
    "#                                 C=logi_result.best_params_['C'], max_iter=logi_result.best_params_['max_iter'],\n",
    "#                                 multi_class=logi_result.best_params_['multi_class'], class_weight = 'balanced')\n",
    "\n",
    "# svm_model = SVC(degree=svm_result.best_params_['degree'], gamma=svm_result.best_params_['gamma'] ,C=svm_result.best_params_['C'], \n",
    "#                 kernel=svm_result.best_params_['kernel'], max_iter=svm_result.best_params_['max_iter'], \n",
    "#                 decision_function_shape=svm_result.best_params_['decision_function_shape'])\n",
    "\n",
    "# rf_model = RandomForestClassifier(criterion=rf_result.best_params_['criterion'], max_depth=rf_result.best_params_['max_depth'],\n",
    "#                                   max_features='log2', min_samples_leaf=rf_result.best_params_['min_samples_leaf'], \n",
    "#                                   min_samples_split=rf_result.best_params_['min_samples_split'], n_estimators=rf_result.best_params_['n_estimators'],\n",
    "#                                   random_state=0)\n",
    "\n",
    "# # logi_f_result = cross_validate(estimator=logi_model, X=X_train_new, y = Y_train, scoring=_scoring, return_train_score=True,cv=5, error_score=\"raise\")\n",
    "\n",
    "# estimators = [('rf', rf_model),('svr', svm_model), ('log', logi_model)]\n",
    "\n",
    "# svm_stacking = StackingClassifier(estimators=estimators, final_estimator=SVC(), n_jobs=-1)\n",
    "# svm_stacking_f_result = cross_validate(estimator=svm_stacking,X=X_train_new, y = Y_train, scoring=_scoring, return_train_score=True,cv=5, error_score=\"raise\", n_jobs =-1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
